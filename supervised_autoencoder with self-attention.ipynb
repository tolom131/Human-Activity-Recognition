{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "assignment_08_solution_latest_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "84bbda367bac7e7bffd9b7890a44d65326aaedad40e5a9021c2651157391b1ef"
    },
    "kernelspec": {
      "display_name": "Python 3.8.5 64-bit ('base': conda)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60ea6f3d48cf4ea1aebe408d26171384": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2db75681b05a4e68bf8f13370f63a4f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c7d7a9b9875e45429483995b100c5601",
              "IPY_MODEL_4247c5f7999940b3851e03d29183f185",
              "IPY_MODEL_e833c667c7d54b34a4f665da9ce63867"
            ]
          }
        },
        "2db75681b05a4e68bf8f13370f63a4f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c7d7a9b9875e45429483995b100c5601": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ee276de56dd1489ea9c1b63717784261",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 14%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8db826cb5fb74b09b287962fd88b4e19"
          }
        },
        "4247c5f7999940b3851e03d29183f185": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_616b029fdce04327939b6f63c309c59e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 200,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 27,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_06e5c2726dc640e59c3102fd46596f48"
          }
        },
        "e833c667c7d54b34a4f665da9ce63867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_83c9ffabc07944c884df791d44fcc666",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 27/200 [02:16&lt;14:34,  5.06s/it]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13b2f51fcf80493f90714106ce417ba0"
          }
        },
        "ee276de56dd1489ea9c1b63717784261": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8db826cb5fb74b09b287962fd88b4e19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "616b029fdce04327939b6f63c309c59e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "06e5c2726dc640e59c3102fd46596f48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "83c9ffabc07944c884df791d44fcc666": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13b2f51fcf80493f90714106ce417ba0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolom131/Human-Activity-Recognition/blob/main/supervised_autoencoder%20with%20self-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQGQXmOpA9J-"
      },
      "source": [
        "# Unsupervised image denoising"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TGBcGR2LA9KI"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNpI2DZ6A9KJ"
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from torchvision.transforms import Compose, ToTensor, ToPILImage, Resize, Lambda, Normalize, Grayscale\n",
        "from torch.utils.data import DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from math import log10\n",
        "from tqdm.notebook import tqdm\n",
        "import os\n",
        "import math\n",
        "import torch as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TiZsA_A9KO"
      },
      "source": [
        "## Hyper parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDynD4nA9KP"
      },
      "source": [
        "device        = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ==================================================\n",
        "# determine optimal hyper-parameters to obtain best testing performance\n",
        "number_epoch    = 200\n",
        "size_minibatch  = 128\n",
        "learning_rate   = 0.01\n",
        "# =================================================="
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FriR4w79i_Z",
        "outputId": "904e423c-b99b-41ee-edd6-26f43b5207e2"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/HAR/'\n",
        "filename_data   = 'WISDM_at_v2.0_raw.txt'\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/HAR/')\n",
        "import wisdm_1_1\n",
        "import wisdm_2_0\n",
        "# x_train, y_train, num_classes = wisdm_1_1.create_wisdm_1_1(directory_data + filename_data)\n",
        "origianl_x, original_y, num_classes = wisdm_2_0.create_wisdm_2_0(directory_data + filename_data)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape :  (14423, 200, 3) y_train.shape:  (14423, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IsbQDqQ9mCD"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(origianl_x, original_y, random_state=42, stratify=original_y, test_size=0.2)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9AbHCZPpA9KR"
      },
      "source": [
        "## Costumize dataloader for pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UveCjrBA9KS"
      },
      "source": [
        "class dataset (Dataset):\n",
        "    def  __init__(self, data, label):\n",
        "\n",
        "        self.data    = data\n",
        "        self.label    = label\n",
        "            \n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data    = self.data[index]\n",
        "        label   = self.label[index]\n",
        "        return (data, label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data.shape[0]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "357alwWgA9KU"
      },
      "source": [
        "## Construct datasets and dataloaders for training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b7JDqjSVA9KU"
      },
      "source": [
        "## transformer를 통과하기 위해 데이터 shape 변경\n",
        "# x_train = x_train.reshape(-1, x_train.shape[2], x_train.shape[1])\n",
        "# x_test = x_test.reshape(-1, x_test.shape[2], x_test.shape[1])\n",
        "x_train = tf.FloatTensor(x_train)\n",
        "x_test = tf.FloatTensor(x_test)\n",
        "x_train = x_train.permute(0, 2, 1)\n",
        "x_test = x_test.permute(0, 2, 1)\n",
        "\n",
        "dataset_train = dataset(x_train, y_train) \n",
        "dataset_test  = dataset(x_test, y_test) \n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, batch_size=size_minibatch, shuffle=True, drop_last=True, num_workers=2)\n",
        "dataloader_test  = DataLoader(dataset_test,  batch_size=size_minibatch, shuffle=False, drop_last=True, num_workers=2) "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yn9jMLrA9KX"
      },
      "source": [
        "## Class for the neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHuu2uZ9SyRm"
      },
      "source": [
        "class my_MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, emb_size=200, num_heads=1):\n",
        "        super().__init__()\n",
        "        self.emb_size = emb_size\n",
        "        self.num_heads = num_heads\n",
        "        self.multihead = nn.MultiheadAttention(emb_size, num_heads, batch_first=True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        # split keys, queries and values in num_heads\n",
        "        x = self.multihead(inputs, inputs, inputs, need_weights=False)\n",
        "        return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yd1LXRHTOgAF"
      },
      "source": [
        "class PositionalEncoding1D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        :param channels: The last dimension of the tensor you want to apply pos emb to.\n",
        "        \"\"\"\n",
        "        super(PositionalEncoding1D, self).__init__()\n",
        "        self.channels = channels\n",
        "        inv_freq = 1. / (10000 ** (torch.arange(0, channels, 2).float() / channels))\n",
        "        self.register_buffer('inv_freq', inv_freq)\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        \"\"\"\n",
        "        :param tensor: A 3d tensor of size (batch_size, x, ch)\n",
        "        :return: Positional Encoding Matrix of size (batch_size, x, ch)\n",
        "        \"\"\"\n",
        "        if len(tensor.shape) != 3:\n",
        "            raise RuntimeError(\"The input tensor has to be 3d!\")\n",
        "        batch_size, x, orig_ch = tensor.shape\n",
        "        pos_x = torch.arange(x, device=tensor.device).type(self.inv_freq.type())\n",
        "        sin_inp_x = torch.einsum(\"i,j->ij\", pos_x, self.inv_freq)\n",
        "        emb_x = torch.cat((sin_inp_x.sin(), sin_inp_x.cos()), dim=-1)\n",
        "        emb = torch.zeros((x,self.channels),device=tensor.device).type(tensor.type())\n",
        "        emb[:,:self.channels] = emb_x\n",
        "\n",
        "        return emb[None,:,:orig_ch].repeat(batch_size, 1, 1)\n",
        "\n",
        "class PositionalEncodingPermute1D(nn.Module):\n",
        "    def __init__(self, channels):\n",
        "        \"\"\"\n",
        "        Accepts (batchsize, ch, x) instead of (batchsize, x, ch)        \n",
        "        \"\"\"\n",
        "        super(PositionalEncodingPermute1D, self).__init__()\n",
        "        self.penc = PositionalEncoding1D(channels)\n",
        "\n",
        "    def forward(self, tensor):\n",
        "        tensor = tensor.permute(0,2,1)\n",
        "        enc = self.penc(tensor)\n",
        "        return enc.permute(0,2,1)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TCHYzLmjd3I"
      },
      "source": [
        "class ScaleDotProductAttention(nn.Module):\n",
        "    '''\n",
        "    Compute scale dot product attention\n",
        "    실질적인 attention score을 계산하는 클래스\n",
        "    '''\n",
        "    def __init__(self):\n",
        "        super(ScaleDotProductAttention,self).__init__()\n",
        "        self.softmax = nn.Softmax()\n",
        "        \n",
        "    def forward(self,q,k,v,mask = None, e = 1e-12):\n",
        "        # input is 4 dimension tensor\n",
        "        # [batch_size,head,length,d_tensor]\n",
        "        batch_size,head,length,d_tensor = k.size()\n",
        "        \n",
        "        # 1. dot product Query with Key^T to compute similarity\n",
        "        k_t = k.view(batch_size,head,d_tensor,length)\n",
        "        score = (q @ k_t) / math.sqrt(d_tensor) # @연산은 np.matmul과 같은 역할\n",
        "        \n",
        "        '''\n",
        "        Note) '@' operator\n",
        "        If either argument is N-D, N > 2, \n",
        "        it is treated as a stack of matrices residing in the last two indexes and broadcast accordingly.\n",
        "        '''\n",
        "        \n",
        "        # 2. applying masking(optional)\n",
        "        if mask is not None:\n",
        "            score = score.masked_fill(mask == 0 ,-e)\n",
        "        \n",
        "        # 3. pass tem softmax to make [0,1] range\n",
        "        score = self.softmax(score)\n",
        "        \n",
        "        # 4. Multiply with Value\n",
        "        v = score @ v\n",
        "        \n",
        "        return v, score\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self,d_model,n_head):\n",
        "        super(MultiHeadAttention,self).__init__()\n",
        "        self.n_head = n_head\n",
        "        self.attention = ScaleDotProductAttention()\n",
        "        self.w_q = nn.Linear(d_model,d_model)\n",
        "        self.w_k = nn.Linear(d_model,d_model)\n",
        "        self.w_v = nn.Linear(d_model,d_model)\n",
        "        self.w_concat = nn.Linear(d_model,d_model)\n",
        "    \n",
        "    def split(self,tensor):\n",
        "        '''\n",
        "        splits tensor by number of head\n",
        "        \n",
        "        param tensor = [batch_size,length,d_model]\n",
        "        out = [batch_size,head,length,d_tensor]\n",
        "        \n",
        "        d_model을 head와 d_tensor로 쪼개는걸로 이해하면 될듯. \n",
        "        d_tensor는 head의 값에 따라 변함.(head값은 정해주는 값이기 때문..)\n",
        "        '''\n",
        "        batch_size,length,d_model = tensor.size()\n",
        "        \n",
        "        d_tensor = d_model//self.n_head\n",
        "        \n",
        "        tensor = tensor.view(batch_size,self.n_head,length,d_tensor)\n",
        "        \n",
        "        return tensor\n",
        "    \n",
        "    def concat(self,tensor):\n",
        "        '''\n",
        "        inverse function of self.split(tensor = torch.Tensor)\n",
        "        \n",
        "        param tensor = [batch_size,head,length,d_tensor]\n",
        "        out = [batch_size,length,d_model]\n",
        "        '''\n",
        "        batch_size,head,length,d_tensor = tensor.size()\n",
        "        d_model = head*d_tensor\n",
        "        \n",
        "        tensor = tensor.view(batch_size,length,d_model)\n",
        "        return tensor\n",
        "    \n",
        "    def forward(self,q,k,v,mask = None):\n",
        "        \n",
        "        #1. dot product with weight metrics\n",
        "        q,k,v = self.w_q(q),self.w_k(k),self.w_v(v)\n",
        "        \n",
        "        # 2. split tensor by number of heads\n",
        "        q,k,v = self.split(q),self.split(k),self.split(v)\n",
        "        \n",
        "        # 3. do scale dot product to compute similarity (attention 계산)\n",
        "        out,attention = self.attention(q,k,v, mask = mask)\n",
        "        \n",
        "        # 4. concat and pass to linear layer\n",
        "        out = self.concat(out)\n",
        "        out = self.w_concat(out)\n",
        "        \n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75QGenrzTEAR"
      },
      "source": [
        "# 수정: 오토 인코더의 인코더처럼 만들기 위해 마지막 Linear에서 그 크기를 줄여준다.\n",
        "class FeedForwardBlock_Encoder(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, reduction=1):\n",
        "        super(FeedForwardBlock_Encoder, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channel, in_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.batch1 = nn.BatchNorm1d(in_channel)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channel, out_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batch2 = nn.BatchNorm1d(out_channel)            \n",
        "\n",
        "        self.pool = nn.MaxPool1d(2, 2)\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.relu1(x)\n",
        "        x = self.batch1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.batch2(x)\n",
        "\n",
        "        if self.reduction == 2:\n",
        "            x = self.pool(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2fieNGYV4xe"
      },
      "source": [
        "# Now create the Transformer Encoder Block\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, dim=768, in_channel=16, out_channel=16, reduction=1, **kwargs):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "\n",
        "        self.laynorm1 = nn.LayerNorm(dim)\n",
        "        self.multihead1 = MultiHeadAttention(dim, 1)\n",
        "\n",
        "        self.laynorm2 = nn.LayerNorm(dim)\n",
        "        self.ff = FeedForwardBlock_Encoder(in_channel, out_channel, reduction=reduction)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        _x = inputs\n",
        "        x = self.laynorm1(inputs)\n",
        "        x = self.multihead1(x, x, x)\n",
        "        x += _x\n",
        "\n",
        "        x = self.laynorm2(x)\n",
        "        x = self.ff(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26QaLC6AWsO9"
      },
      "source": [
        "# 수정: 오토 인코더의 인코더처럼 만들기 위해 마지막 Linear에서 그 크기를 줄여준다.\n",
        "class FeedForwardBlock_Decoder(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, amp=1):\n",
        "        super(FeedForwardBlock_Decoder, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channel, in_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.batch1 = nn.BatchNorm1d(in_channel)\n",
        "        \n",
        "        self.conv2 = nn.Conv1d(in_channel, out_channel, kernel_size=3, padding=1, stride=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.batch2 = nn.BatchNorm1d(out_channel)            \n",
        "\n",
        "        self.pool = nn.Upsample(scale_factor=2, mode=\"nearest\")\n",
        "        self.amp = amp\n",
        "\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.relu1(x)\n",
        "        x = self.batch1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.batch2(x)\n",
        "\n",
        "        if self.amp == 2:\n",
        "            x = self.pool(x)\n",
        "        return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6ys44YMV2bL"
      },
      "source": [
        "# Now create the Transformer Encoder Block\n",
        "class TransformerDecoderBlock(nn.Module):\n",
        "    def __init__(self, dim=768, in_channel=16, out_channel=16, amp=1, **kwargs):\n",
        "        super(TransformerDecoderBlock, self).__init__()\n",
        "\n",
        "        self.laynorm1 = nn.LayerNorm(dim)\n",
        "        self.multihead1 = MultiHeadAttention(dim, 1)\n",
        "\n",
        "        self.laynorm2 = nn.LayerNorm(dim)\n",
        "        self.ff =  FeedForwardBlock_Decoder(in_channel, out_channel, amp)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        _x = inputs\n",
        "        x = self.laynorm1(inputs)\n",
        "        x = self.multihead1(x, x, x)\n",
        "        x += _x\n",
        "\n",
        "        x = self.laynorm2(x)\n",
        "        x = self.ff(x)\n",
        "        return x"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VPt0SXOTuS3"
      },
      "source": [
        "class Conseuctive_Conv(nn.Sequential):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__(\n",
        "            nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(dim, dim, kernel_size=3, stride=1, padding=1)\n",
        "        )\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, dim, channel):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.channel = channel\n",
        "\n",
        "        # self.conv1 = Conseuctive_Conv(channel*20)\n",
        "        # self.conv2 = Conseuctive_Conv(channel*40)\n",
        "\n",
        "        self.encoder12 = TransformerEncoderBlock(dim=dim,     in_channel=channel,    out_channel=channel*20,  reduction=2)\n",
        "        self.encoder22 = TransformerEncoderBlock(dim=dim//2,  in_channel=channel*20, out_channel=channel*40,  reduction=2)\n",
        "        self.encoder32 = TransformerEncoderBlock(dim=dim//4,  in_channel=channel*40, out_channel=channel*80,  reduction=2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        x = self.encoder12(inputs)    \n",
        "        x = self.encoder22(x)\n",
        "        x = self.encoder32(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, dim, channel):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.channel = channel\n",
        "\n",
        "        # self.conv3 = Conseuctive_Conv(channel*40)\n",
        "        # self.conv4 = Conseuctive_Conv(channel*20)\n",
        "\n",
        "        self.decoder12 = TransformerDecoderBlock(dim=dim//8,  in_channel=channel*80, out_channel=channel*40, amp=2)\n",
        "        self.decoder22 = TransformerDecoderBlock(dim=dim//4,  in_channel=channel*40, out_channel=channel*20, amp=2)\n",
        "        self.decoder32 = TransformerDecoderBlock(dim=dim//2,  in_channel=channel*20, out_channel=channel,    amp=2)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        x = self.decoder12(inputs)\n",
        "        x = self.decoder22(x)\n",
        "        x = self.decoder32(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, dim, channel, num_classes, is_alone=False):\n",
        "        super(Classifier, self).__init__()\n",
        "        \n",
        "        if is_alone:\n",
        "            self.dim = dim\n",
        "            self.channel = channel\n",
        "            self.num_classes = num_classes\n",
        "        else:\n",
        "            self.dim = dim//8\n",
        "            self.channel = channel*80\n",
        "            self.num_classes = num_classes\n",
        "\n",
        "        # original inputs shape : [batch_size, channel, dim]\n",
        "        # classifier inputs shape : [batch_size, channel*80, dim//8]\n",
        "\n",
        "        self.conv1 = nn.Conv1d(self.channel, 64, kernel_size=3, padding=1, stride=1)\n",
        "        self.batch1 = nn.BatchNorm1d(64)\n",
        "        self.relu1 = nn.ReLU()\n",
        "\n",
        "        self.conv2 = nn.Conv1d(64, 128, kernel_size=3, padding=1, stride=1)\n",
        "        self.batch2 = nn.BatchNorm1d(128)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        \n",
        "        self.lstm = nn.LSTM(self.dim, self.dim, batch_first=True)\n",
        "        self.classifier = nn.Linear(128, self.num_classes)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "\n",
        "        x = self.conv1(inputs)\n",
        "        x = self.batch1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batch2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x, _ = self.lstm(x)\n",
        "        x = x[:, :, -1]         # return_sequences = False\n",
        "        x = self.classifier(x)\n",
        "        x = self.softmax(x)\n",
        "\n",
        "        return x  \n",
        "\n",
        "class SupervisedAutoencoder(nn.Module):\n",
        "    def __init__(self, dim, channel, num_classes, device):\n",
        "        super(SupervisedAutoencoder, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.channel = channel\n",
        "\n",
        "        self.posencoding = PositionalEncodingPermute1D(channels=channel+1)\n",
        "        self.encoder = Encoder(dim, channel)\n",
        "        self.decoder = Decoder(dim, channel)\n",
        "\n",
        "        self.classifier = Classifier(dim, channel, num_classes) \n",
        "\n",
        "    def forward(self, inputs):\n",
        "    \n",
        "        x = self.posencoding(inputs)\n",
        "        x = self.encoder(x)\n",
        "        classified = self.classifier(x)\n",
        "        decoded = self.decoder(x)\n",
        "        return classified, decoded"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uig2PSDaTDu6"
      },
      "source": [
        "## model summary 용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXyFhOErkMRr"
      },
      "source": [
        "# p_enc_1d = PositionalEncodingPermute1D(4)\n",
        "# x = torch.zeros((2,3,200))\n",
        "# p_enc_1d(x).shape"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZmCOhVRTAUly"
      },
      "source": [
        "# from torchsummary import summary as summary_\n",
        "\n",
        "# model = SupervisedAutoencoder(200, 3, 6, device).to(device)\n",
        "# summary_(model, (3, 200))"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8DSwKtMOqBH"
      },
      "source": [
        "## 모델링 과정을 위해 필요한 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCOI_G1tnmUE"
      },
      "source": [
        "def compute_prediction(model, input):\n",
        "    prediction, decoded = model(input)\n",
        "    return prediction, decoded"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA0KNRpEnnN6"
      },
      "source": [
        "def compute_mse_loss(input, prediction):\n",
        "    \n",
        "    mse = nn.MSELoss()\n",
        "    \n",
        "    # ==================================================\n",
        "    # fill up the blank\n",
        "    loss_mse = mse(prediction, input)\n",
        "    # ==================================================\n",
        "    \n",
        "    loss_mse_value = loss_mse.item() \n",
        "    \n",
        "    return loss_mse, loss_mse_value"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT9FgvXjcVBB"
      },
      "source": [
        "def compute_entropy_loss(inputs, prediction):\n",
        "    cross = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_cross = cross(prediction, inputs)\n",
        "\n",
        "    loss_cross_value = loss_cross.item()\n",
        "    return loss_cross, loss_cross_value"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlSY2TrROn6W"
      },
      "source": [
        "def compute_accuracy(prediction, label):\n",
        "    # ================================================================================ \n",
        "    # complete the function body\n",
        "    b_Prediction = torch.argmax(prediction, 1)\n",
        "    b_label = torch.argmax(label, 1)\n",
        "    bCorrect = (b_Prediction == b_label)\n",
        "    accuracy = bCorrect.float().mean() * 100\n",
        "    # ================================================================================ \n",
        "\n",
        "    return accuracy"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DYCbEtyPHW5"
      },
      "source": [
        "## 모델링 과정 정리를 위한 배열"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2GV2m67PGMD"
      },
      "source": [
        "loss_mean_train     = np.zeros(number_epoch)\n",
        "loss_std_train      = np.zeros(number_epoch)\n",
        "accuracy_mean_train = np.zeros(number_epoch)\n",
        "accuracy_std_train  = np.zeros(number_epoch)\n",
        "\n",
        "loss_mean_test      = np.zeros(number_epoch)\n",
        "loss_std_test       = np.zeros(number_epoch)\n",
        "accuracy_mean_test  = np.zeros(number_epoch)\n",
        "accuracy_std_test   = np.zeros(number_epoch)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTptbDGZA9KZ"
      },
      "source": [
        "## Build the network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MgUbMJdA9KZ"
      },
      "source": [
        "# model = Classifier(200, 3, 6, is_alone=True).to(device)\n",
        "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzsj97WB3_tq"
      },
      "source": [
        "model = SupervisedAutoencoder(200, 3, 6, device).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4180lAgNxK7",
        "outputId": "f216c84c-1e24-45c5-adde-637f5cda7304",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "def weight_init_xavier_uniform(submodule):\n",
        "    if isinstance(submodule, torch.nn.Conv1d) or isinstance(submodule, torch.nn.Linear):\n",
        "        torch.nn.init.xavier_uniform_(submodule.weight)\n",
        "model.apply(weight_init_xavier_uniform)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SupervisedAutoencoder(\n",
              "  (posencoding): PositionalEncodingPermute1D(\n",
              "    (penc): PositionalEncoding1D()\n",
              "  )\n",
              "  (encoder): Encoder(\n",
              "    (encoder12): TransformerEncoderBlock(\n",
              "      (laynorm1): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (w_k): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (w_v): Linear(in_features=200, out_features=200, bias=True)\n",
              "        (w_concat): Linear(in_features=200, out_features=200, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((200,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Encoder(\n",
              "        (conv1): Conv1d(3, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(3, 60, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (encoder22): TransformerEncoderBlock(\n",
              "      (laynorm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_k): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_v): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_concat): Linear(in_features=100, out_features=100, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Encoder(\n",
              "        (conv1): Conv1d(60, 60, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(60, 120, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "    (encoder32): TransformerEncoderBlock(\n",
              "      (laynorm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_k): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_v): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_concat): Linear(in_features=50, out_features=50, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Encoder(\n",
              "        (conv1): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(120, 240, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (decoder12): TransformerDecoderBlock(\n",
              "      (laynorm1): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=25, out_features=25, bias=True)\n",
              "        (w_k): Linear(in_features=25, out_features=25, bias=True)\n",
              "        (w_v): Linear(in_features=25, out_features=25, bias=True)\n",
              "        (w_concat): Linear(in_features=25, out_features=25, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((25,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Decoder(\n",
              "        (conv1): Conv1d(240, 240, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(240, 120, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): Upsample(scale_factor=2.0, mode=nearest)\n",
              "      )\n",
              "    )\n",
              "    (decoder22): TransformerDecoderBlock(\n",
              "      (laynorm1): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_k): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_v): Linear(in_features=50, out_features=50, bias=True)\n",
              "        (w_concat): Linear(in_features=50, out_features=50, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((50,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Decoder(\n",
              "        (conv1): Conv1d(120, 120, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(120, 60, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): Upsample(scale_factor=2.0, mode=nearest)\n",
              "      )\n",
              "    )\n",
              "    (decoder32): TransformerDecoderBlock(\n",
              "      (laynorm1): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "      (multihead1): MultiHeadAttention(\n",
              "        (attention): ScaleDotProductAttention(\n",
              "          (softmax): Softmax(dim=None)\n",
              "        )\n",
              "        (w_q): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_k): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_v): Linear(in_features=100, out_features=100, bias=True)\n",
              "        (w_concat): Linear(in_features=100, out_features=100, bias=True)\n",
              "      )\n",
              "      (laynorm2): LayerNorm((100,), eps=1e-05, elementwise_affine=True)\n",
              "      (ff): FeedForwardBlock_Decoder(\n",
              "        (conv1): Conv1d(60, 60, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu1): ReLU()\n",
              "        (batch1): BatchNorm1d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv1d(60, 3, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "        (relu2): ReLU()\n",
              "        (batch2): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (pool): Upsample(scale_factor=2.0, mode=nearest)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (conv1): Conv1d(240, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (batch1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu1): ReLU()\n",
              "    (conv2): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
              "    (batch2): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu2): ReLU()\n",
              "    (lstm): LSTM(25, 25, batch_first=True)\n",
              "    (classifier): Linear(in_features=128, out_features=6, bias=True)\n",
              "    (softmax): Softmax(dim=1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NKn_pSx3L0H"
      },
      "source": [
        "## Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "60ea6f3d48cf4ea1aebe408d26171384",
            "2db75681b05a4e68bf8f13370f63a4f4",
            "c7d7a9b9875e45429483995b100c5601",
            "4247c5f7999940b3851e03d29183f185",
            "e833c667c7d54b34a4f665da9ce63867",
            "ee276de56dd1489ea9c1b63717784261",
            "8db826cb5fb74b09b287962fd88b4e19",
            "616b029fdce04327939b6f63c309c59e",
            "06e5c2726dc640e59c3102fd46596f48",
            "83c9ffabc07944c884df791d44fcc666",
            "13b2f51fcf80493f90714106ce417ba0"
          ]
        },
        "id": "UYEYvwqKmz-V",
        "outputId": "d6e55aac-b8c6-4841-a405-b6ee56886c5e"
      },
      "source": [
        "# ================================================================================\n",
        "# \n",
        "# iterations for epochs\n",
        "#\n",
        "# ================================================================================\n",
        "for i in tqdm(range(number_epoch)):\n",
        "    \n",
        "    # ================================================================================\n",
        "    # \n",
        "    # training\n",
        "    #\n",
        "    # ================================================================================\n",
        "    loss_train_epoch        = []\n",
        "    accuracy_train_epoch    = []\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for index_batch, (data, label) in enumerate(dataloader_train):\n",
        "\n",
        "        data_train = data.to(device)\n",
        "        label_train = label.to(device)\n",
        "        \n",
        "        classified, decoded = compute_prediction(model, data_train)\n",
        "\n",
        "        # classified loss\n",
        "        classified_loss, classfied_loss_value       = compute_entropy_loss(label_train, classified)\n",
        "\n",
        "        # decoded loss\n",
        "        decoded_loss, decoded_loss_value            = compute_mse_loss(data_train, decoded)\n",
        "\n",
        "        # classified accuracy\n",
        "        accuracy_train                              = compute_accuracy(classified, label_train).to(\"cpu\")\n",
        "        accuracy_train = accuracy_train.numpy()\n",
        "\n",
        "        loss_train = classified_loss * 0.1 + decoded_loss * 0.9\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_train_epoch.append(classfied_loss_value)\n",
        "        accuracy_train_epoch.append(accuracy_train)\n",
        "\n",
        "    loss_mean_train[i]      = np.mean(loss_train_epoch)\n",
        "    loss_std_train[i]       = np.std(loss_train_epoch)\n",
        "\n",
        "    accuracy_mean_train[i]  = np.mean(accuracy_train_epoch)\n",
        "    accuracy_std_train[i]   = np.std(accuracy_train_epoch)\n",
        "\n",
        "    # ================================================================================\n",
        "    # \n",
        "    # testing\n",
        "    #\n",
        "    # ================================================================================\n",
        "    loss_test_epoch        = []\n",
        "    accuracy_test_epoch    = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "\n",
        "    for index_batch, (data, label) in enumerate(dataloader_test):\n",
        "\n",
        "        data_test = data.to(device)\n",
        "        label_test = label.to(device)\n",
        "        \n",
        "        classified, decoded             = compute_prediction(model, data_test)\n",
        "\n",
        "        # classified loss\n",
        "        classified_loss, classfied_loss_value       = compute_entropy_loss(label_test, classified)\n",
        "\n",
        "        # decoded loss\n",
        "        decoded_loss, decoded_loss_value            = compute_mse_loss(data_test, decoded)\n",
        "\n",
        "        # classified accuracy\n",
        "        accuracy_test               = compute_accuracy(classified, label_test).to(\"cpu\")\n",
        "        accuracy_test = accuracy_test.numpy()\n",
        "\n",
        "        loss_test_epoch.append(classfied_loss_value)\n",
        "        accuracy_test_epoch.append(accuracy_test)\n",
        "\n",
        "    loss_mean_test[i]      = np.mean(loss_test_epoch)\n",
        "    loss_std_test[i]       = np.std(loss_test_epoch)\n",
        "\n",
        "    accuracy_mean_test[i]  = np.mean(accuracy_test_epoch)\n",
        "    accuracy_std_test[i]   = np.std(accuracy_test_epoch)\n",
        "\n",
        "    print(f\"epoch : {i}, train acc : {np.mean(accuracy_train_epoch)}, train loss : {np.mean(loss_train_epoch)}\")\n",
        "    print(f\"epoch : {i}, test acc : {np.mean(accuracy_test_epoch)}, test loss : {np.mean(loss_test_epoch)}\")\n",
        "    print()\n",
        "\n",
        "    # print(f\"epoch : {i}, train loss : {np.mean(loss_train_epoch)}\")\n",
        "    # print(f\"epoch : {i}, test loss : {np.mean(loss_test_epoch)}\")\n",
        "    # print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60ea6f3d48cf4ea1aebe408d26171384",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:30: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0, train acc : 41.67534637451172, train loss : 1.6244662139150832\n",
            "epoch : 0, test acc : 42.5426139831543, test loss : 1.618160767988725\n",
            "\n",
            "epoch : 1, train acc : 42.51736068725586, train loss : 1.618411695957184\n",
            "epoch : 1, test acc : 42.5426139831543, test loss : 1.618154303594069\n",
            "\n",
            "epoch : 2, train acc : 42.53472137451172, train loss : 1.618206919564141\n",
            "epoch : 2, test acc : 42.5426139831543, test loss : 1.61799818277359\n",
            "\n",
            "epoch : 3, train acc : 42.54340362548828, train loss : 1.6181683619817098\n",
            "epoch : 3, test acc : 42.5426139831543, test loss : 1.6181505864316768\n",
            "\n",
            "epoch : 4, train acc : 42.54340362548828, train loss : 1.6181615736749437\n",
            "epoch : 4, test acc : 42.5426139831543, test loss : 1.6181514100594954\n",
            "\n",
            "epoch : 5, train acc : 42.55208206176758, train loss : 1.6180240035057067\n",
            "epoch : 5, test acc : 42.5426139831543, test loss : 1.617945286360654\n",
            "\n",
            "epoch : 6, train acc : 42.58680725097656, train loss : 1.6178049776289198\n",
            "epoch : 6, test acc : 42.5426139831543, test loss : 1.6181655526161194\n",
            "\n",
            "epoch : 7, train acc : 42.55208206176758, train loss : 1.618070900440216\n",
            "epoch : 7, test acc : 42.5426139831543, test loss : 1.6181656230579724\n",
            "\n",
            "epoch : 8, train acc : 42.58680725097656, train loss : 1.6177237060334948\n",
            "epoch : 8, test acc : 42.5426139831543, test loss : 1.618165590546348\n",
            "\n",
            "epoch : 9, train acc : 42.55208206176758, train loss : 1.6180708461337618\n",
            "epoch : 9, test acc : 42.5426139831543, test loss : 1.6181656176393682\n",
            "\n",
            "epoch : 10, train acc : 42.55208206176758, train loss : 1.6180708818965488\n",
            "epoch : 10, test acc : 42.5426139831543, test loss : 1.6181655471975154\n",
            "\n",
            "epoch : 11, train acc : 42.578125, train loss : 1.6178104678789775\n",
            "epoch : 11, test acc : 42.5426139831543, test loss : 1.6181655255230991\n",
            "\n",
            "epoch : 12, train acc : 42.54340362548828, train loss : 1.618157728513082\n",
            "epoch : 12, test acc : 42.5426139831543, test loss : 1.6181656609882007\n",
            "\n",
            "epoch : 13, train acc : 42.51736068725586, train loss : 1.6184181319342719\n",
            "epoch : 13, test acc : 42.5426139831543, test loss : 1.6181654117324136\n",
            "\n",
            "epoch : 14, train acc : 42.53472137451172, train loss : 1.618244473139445\n",
            "epoch : 14, test acc : 42.5426139831543, test loss : 1.6181655201044949\n",
            "\n",
            "epoch : 15, train acc : 42.56944274902344, train loss : 1.6178972297244603\n",
            "epoch : 15, test acc : 42.5426139831543, test loss : 1.618165595964952\n",
            "\n",
            "epoch : 16, train acc : 42.56076431274414, train loss : 1.6179840193854438\n",
            "epoch : 16, test acc : 42.5426139831543, test loss : 1.6181654279882258\n",
            "\n",
            "epoch : 17, train acc : 42.56944274902344, train loss : 1.6178972270753649\n",
            "epoch : 17, test acc : 42.5426139831543, test loss : 1.6181656609882007\n",
            "\n",
            "epoch : 18, train acc : 42.578125, train loss : 1.6178104082743328\n",
            "epoch : 18, test acc : 42.5426139831543, test loss : 1.6181654930114746\n",
            "\n",
            "epoch : 19, train acc : 42.53472137451172, train loss : 1.6182443737983703\n",
            "epoch : 19, test acc : 42.5426139831543, test loss : 1.6181655797091397\n",
            "\n",
            "epoch : 20, train acc : 42.53472137451172, train loss : 1.6182443777720132\n",
            "epoch : 20, test acc : 42.5426139831543, test loss : 1.6181654008952053\n",
            "\n",
            "epoch : 21, train acc : 42.56076431274414, train loss : 1.6179839465353223\n",
            "epoch : 21, test acc : 42.5426139831543, test loss : 1.6181654659184543\n",
            "\n",
            "epoch : 22, train acc : 42.52604293823242, train loss : 1.6183311223983765\n",
            "epoch : 22, test acc : 42.5426139831543, test loss : 1.6181654984300786\n",
            "\n",
            "epoch : 23, train acc : 42.52604293823242, train loss : 1.6183311078283522\n",
            "epoch : 23, test acc : 42.5426139831543, test loss : 1.6181653412905606\n",
            "\n",
            "epoch : 24, train acc : 42.53472137451172, train loss : 1.6182441896862454\n",
            "epoch : 24, test acc : 42.5426139831543, test loss : 1.6181652654301037\n",
            "\n",
            "epoch : 25, train acc : 42.54340362548828, train loss : 1.6181572569741143\n",
            "epoch : 25, test acc : 42.5426139831543, test loss : 1.6181650053371082\n",
            "\n",
            "epoch : 26, train acc : 42.54340362548828, train loss : 1.618156952328152\n",
            "epoch : 26, test acc : 42.5426139831543, test loss : 1.6181642792441628\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LULSrZv-UTDz"
      },
      "source": [
        "## Plot 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "08FdiEjDV0kb"
      },
      "source": [
        "def plot_curve_error(data_mean, data_std, x_label, y_label, title):\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.title(title)\n",
        "\n",
        "    alpha = 0.3\n",
        "    \n",
        "    plt.plot(range(len(data_mean)), data_mean, '-', color = 'red')\n",
        "    plt.fill_between(range(len(data_mean)), data_mean - data_std, data_mean + data_std, facecolor = 'blue', alpha = alpha) \n",
        "    \n",
        "    plt.xlabel(x_label)\n",
        "    plt.ylabel(y_label)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ENxj-BQUZ-7"
      },
      "source": [
        "print('[plot the training loss]')\n",
        "print('') \n",
        "plot_curve_error(loss_mean_train, loss_std_train, 'epoch', 'loss', 'loss (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bqK2U_jWXva"
      },
      "source": [
        "print('[plot the testing loss]')\n",
        "print('') \n",
        "plot_curve_error(loss_mean_test, loss_std_test, 'epoch', 'loss', 'loss (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_J600wqOWgNF"
      },
      "source": [
        "print('[plot the traning accuracy]') \n",
        "print('') \n",
        "plot_curve_error(accuracy_mean_train, accuracy_std_train, 'epoch', 'accuracy', 'Accuracy (training)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsVmJXxnWIl9"
      },
      "source": [
        "print('[plot the testing accuracy]') \n",
        "print('') \n",
        "plot_curve_error(accuracy_mean_test, accuracy_std_test, 'epoch', 'accuracy', 'Accuracy (testing)')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aF8p8kkmUasj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}