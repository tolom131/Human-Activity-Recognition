{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_supervised_ensemble_autoencoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOC3XvYrQr5sXLPZSdRe8O8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolom131/Human-Activity-Recognition/blob/main/tensorflow_supervised_ensemble_autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5s9FOs7OKyE"
      },
      "source": [
        "# from __future__ import print_function\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, LSTM, Conv1D, Activation, MaxPooling1D, UpSampling1D, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import random as rn\n",
        "from resource import *\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65m2C00d4uNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d66d45-9398-4d63-e75f-05e30269b1f3"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/HAR/'\n",
        "filename_data   = 'WISDM_at_v2.0_raw.txt'\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/HAR/')\n",
        "import wisdm_2_0\n",
        "# x_train, y_train, num_classes = wisdm_1_1.create_wisdm_1_1(directory_data + filename_data)\n",
        "origianl_x, original_y, num_classes = wisdm_2_0.create_wisdm_2_0(directory_data + filename_data)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape :  (13913, 200, 3) y_train.shape:  (13913, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-iVEk_mMtqi"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "def shift(x, y):\n",
        "    new = []\n",
        "    for i in range(5):\n",
        "        temp = np.roll(x, i*40, axis=1)\n",
        "        new.extend(temp)\n",
        "    x = np.array(new)\n",
        "    y = np.concatenate([y]*5)\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x,y\n",
        "\n",
        "from scipy.interpolate import CubicSpline\n",
        "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
        "    xx = (np.ones((X.shape[1], 1))*(np.arange(0, X.shape[0], (X.shape[0]-1)/(knot+1)))).transpose()\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, X.shape[1]))\n",
        "    x_range = np.arange(X.shape[0])\n",
        "\n",
        "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
        "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
        "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
        "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
        "\n",
        "def DistortTimesteps(X, sigma=0.2):\n",
        "    tt = GenerateRandomCurves(X, sigma)\n",
        "    tt_cum = np.cumsum(tt, axis=0)\n",
        "\n",
        "    t_scale = [(X.shape[0]-1)/tt_cum[-1, 0], (X.shape[0]-1)/tt_cum[-1, 1], (X.shape[0]-1)/tt_cum[-1, 2]]\n",
        "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
        "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
        "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
        "\n",
        "    return tt_cum\n",
        "\n",
        "def DA_TimeWarp(X, sigma=0.2):\n",
        "    tt_new = DistortTimesteps(X, sigma)\n",
        "    X_new = np.zeros(X.shape)\n",
        "    x_range = np.arange(X.shape[0])\n",
        "\n",
        "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
        "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
        "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
        "\n",
        "    return X_new\n",
        "\n",
        "def TimeWarp(X, Y):\n",
        "    for i in range(X.shape[0]):\n",
        "        data = X[i, :, :]\n",
        "        if i == 0:\n",
        "            trans_list = DA_TimeWarp(data).reshape(-1, X.shape[1], X.shape[2])\n",
        "        else:\n",
        "            trans = DA_TimeWarp(data).reshape(-1, X.shape[1], X.shape[2])\n",
        "            trans_list = np.concatenate([trans_list, trans], axis=0)\n",
        "\n",
        "    return trans_list, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSs2ExbNPOHD"
      },
      "source": [
        "def clssifier_without_ae(x_train, y_train, x_val, y_val):\n",
        "    \n",
        "    initializer = initializers.GlorotNormal()\n",
        "    earlystop = EarlyStopping(patience=50, monitor='val_loss', mode='min')\n",
        "    checkpoint = ModelCheckpoint(\"SAEwithout.h5\", verbose = 0, mode=\"min\", save_best_only=True, save_weights_only = True)\n",
        "    callbacks_list = [checkpoint ]\n",
        "    def make_adam(rates = 0.01):\n",
        "        return keras.optimizers.Adam(learning_rate=rates)\n",
        "\n",
        "    adam = make_adam()\n",
        "    inputs = Input(shape=(TIME_PERIODS, N_FEATURES))    \n",
        "    x = Conv1D(filters=16, kernel_size=3, kernel_initializer=initializer)(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv1D(filters=32, kernel_size=3, kernel_initializer=initializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = LSTM(128, kernel_initializer=initializer)(x)\n",
        "    classified = Dense(num_classes, activation=\"softmax\", name=\"classified\", kernel_initializer=initializer)(x)\n",
        "\n",
        "    model = Model(inputs, classified)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs=300, verbose=1, batch_size = 128, callbacks=callbacks_list, validation_data=(x_val, y_val))\n",
        "    return history, model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ9h7-9R90kM"
      },
      "source": [
        "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
        "    x = keras.layers.Input(shape=(input_shape))\n",
        "    conv_x = keras.layers.BatchNormalization()(x)\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "     \n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps, 1, 1,padding='same')(x)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
        "    print ('Merging skip connection')\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "     \n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    full = keras.layers.GlobalAveragePooling1D()(y)\n",
        "    out = keras.layers.Dense(nb_classes, activation='softmax', name=\"classified\")(full)\n",
        "    return x, out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-5ztN4VPT3m",
        "outputId": "da94df9d-7a1d-438a-df4d-21ab6061acf2"
      },
      "source": [
        "# fix the random seed\n",
        "# seed_sum = 42\n",
        "# np.random.seed(seed_sum)\n",
        "# rn.seed(seed_sum)\n",
        "# tf.random.set_seed(seed_sum)\n",
        "\n",
        "TIME_PERIODS = 200\n",
        "STEP = 200\n",
        "N_FEATURES = 3\n",
        "\n",
        "\n",
        "# 80%, 10%, 10%로 set 나누기\n",
        "x_train, x_val, y_train, y_val = train_test_split(origianl_x, original_y, test_size=0.2, stratify=original_y)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, stratify=y_val)\n",
        "\n",
        "x_timewarp, y_timewarp  = TimeWarp(x_train, y_train)\n",
        "x_train,    y_train     = shift(x_train, y_train)\n",
        "x_train = np.concatenate([x_timewarp, x_train], axis=0)\n",
        "y_train = np.concatenate([y_timewarp, y_train], axis=0)\n",
        "print(\"shape of x_train : \", x_train.shape)\n",
        "\n",
        "# history, model = clssifier_without_ae(x_train, y_train, x_val, y_val)\n",
        "# model.load_weights(\"SAEwithout.h5\")\n",
        "# test_results = model.evaluate(x_test, y_test)\n",
        "\n",
        "x , y = build_resnet((200, 3), 64, 6)\n",
        "model = keras.models.Model(inputs=x, outputs=y)\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=150, verbose=1, validation_data=(x_val, y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of x_train :  (66780, 200, 3)\n",
            "Merging skip connection\n",
            "Epoch 1/150\n",
            "522/522 [==============================] - 25s 41ms/step - loss: 0.5088 - accuracy: 0.7950 - val_loss: 0.4678 - val_accuracy: 0.8009\n",
            "Epoch 2/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.4195 - accuracy: 0.8241 - val_loss: 0.4212 - val_accuracy: 0.8037\n",
            "Epoch 3/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.3759 - accuracy: 0.8401 - val_loss: 0.3916 - val_accuracy: 0.8260\n",
            "Epoch 4/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.3742 - accuracy: 0.8420 - val_loss: 0.4180 - val_accuracy: 0.8181\n",
            "Epoch 5/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.3431 - accuracy: 0.8517 - val_loss: 0.3740 - val_accuracy: 0.8188\n",
            "Epoch 6/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.3289 - accuracy: 0.8572 - val_loss: 0.6624 - val_accuracy: 0.8095\n",
            "Epoch 7/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.3078 - accuracy: 0.8674 - val_loss: 0.3384 - val_accuracy: 0.8411\n",
            "Epoch 8/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.2931 - accuracy: 0.8760 - val_loss: 1.8746 - val_accuracy: 0.7793\n",
            "Epoch 9/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.2720 - accuracy: 0.8899 - val_loss: 0.3385 - val_accuracy: 0.8497\n",
            "Epoch 10/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.2539 - accuracy: 0.8993 - val_loss: 0.6710 - val_accuracy: 0.8382\n",
            "Epoch 11/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.2434 - accuracy: 0.9058 - val_loss: 1.2331 - val_accuracy: 0.7800\n",
            "Epoch 12/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.2323 - accuracy: 0.9113 - val_loss: 0.2573 - val_accuracy: 0.8627\n",
            "Epoch 13/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.2303 - accuracy: 0.9117 - val_loss: 0.6468 - val_accuracy: 0.8670\n",
            "Epoch 14/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.2209 - accuracy: 0.9158 - val_loss: 0.3227 - val_accuracy: 0.8454\n",
            "Epoch 15/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.2095 - accuracy: 0.9222 - val_loss: 0.7035 - val_accuracy: 0.8382\n",
            "Epoch 16/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.2005 - accuracy: 0.9249 - val_loss: 0.3008 - val_accuracy: 0.8871\n",
            "Epoch 17/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.2048 - accuracy: 0.9230 - val_loss: 0.3988 - val_accuracy: 0.8598\n",
            "Epoch 18/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1979 - accuracy: 0.9249 - val_loss: 1.0176 - val_accuracy: 0.7944\n",
            "Epoch 19/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1937 - accuracy: 0.9278 - val_loss: 0.8413 - val_accuracy: 0.7987\n",
            "Epoch 20/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1809 - accuracy: 0.9320 - val_loss: 0.3023 - val_accuracy: 0.8771\n",
            "Epoch 21/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1763 - accuracy: 0.9341 - val_loss: 0.5204 - val_accuracy: 0.8009\n",
            "Epoch 22/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1687 - accuracy: 0.9380 - val_loss: 0.7364 - val_accuracy: 0.8490\n",
            "Epoch 23/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1615 - accuracy: 0.9397 - val_loss: 0.5258 - val_accuracy: 0.8454\n",
            "Epoch 24/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1540 - accuracy: 0.9419 - val_loss: 0.3864 - val_accuracy: 0.8541\n",
            "Epoch 25/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1449 - accuracy: 0.9457 - val_loss: 0.5132 - val_accuracy: 0.8023\n",
            "Epoch 26/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1458 - accuracy: 0.9454 - val_loss: 0.8848 - val_accuracy: 0.7994\n",
            "Epoch 27/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1349 - accuracy: 0.9483 - val_loss: 0.3940 - val_accuracy: 0.8713\n",
            "Epoch 28/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1281 - accuracy: 0.9515 - val_loss: 3.7325 - val_accuracy: 0.7872\n",
            "Epoch 29/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1261 - accuracy: 0.9536 - val_loss: 0.4936 - val_accuracy: 0.8634\n",
            "Epoch 30/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1289 - accuracy: 0.9516 - val_loss: 0.3495 - val_accuracy: 0.8720\n",
            "Epoch 31/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1199 - accuracy: 0.9547 - val_loss: 1.6778 - val_accuracy: 0.8023\n",
            "Epoch 32/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1180 - accuracy: 0.9556 - val_loss: 1.9035 - val_accuracy: 0.8116\n",
            "Epoch 33/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1128 - accuracy: 0.9568 - val_loss: 0.4424 - val_accuracy: 0.8613\n",
            "Epoch 34/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1099 - accuracy: 0.9606 - val_loss: 0.8755 - val_accuracy: 0.8735\n",
            "Epoch 35/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.1042 - accuracy: 0.9613 - val_loss: 1.4315 - val_accuracy: 0.7930\n",
            "Epoch 36/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1047 - accuracy: 0.9607 - val_loss: 1.5064 - val_accuracy: 0.7958\n",
            "Epoch 37/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.1023 - accuracy: 0.9612 - val_loss: 0.3862 - val_accuracy: 0.8289\n",
            "Epoch 38/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0956 - accuracy: 0.9640 - val_loss: 0.5170 - val_accuracy: 0.8807\n",
            "Epoch 39/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0963 - accuracy: 0.9637 - val_loss: 0.3128 - val_accuracy: 0.8613\n",
            "Epoch 40/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0945 - accuracy: 0.9638 - val_loss: 0.3989 - val_accuracy: 0.8763\n",
            "Epoch 41/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0932 - accuracy: 0.9641 - val_loss: 0.7820 - val_accuracy: 0.8828\n",
            "Epoch 42/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0874 - accuracy: 0.9664 - val_loss: 2.8933 - val_accuracy: 0.8196\n",
            "Epoch 43/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0826 - accuracy: 0.9680 - val_loss: 4.0754 - val_accuracy: 0.8001\n",
            "Epoch 44/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0869 - accuracy: 0.9668 - val_loss: 0.4218 - val_accuracy: 0.8778\n",
            "Epoch 45/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.0878 - accuracy: 0.9668 - val_loss: 0.6244 - val_accuracy: 0.8886\n",
            "Epoch 46/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0806 - accuracy: 0.9689 - val_loss: 0.5457 - val_accuracy: 0.8735\n",
            "Epoch 47/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0844 - accuracy: 0.9678 - val_loss: 0.3962 - val_accuracy: 0.8677\n",
            "Epoch 48/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0811 - accuracy: 0.9694 - val_loss: 0.3918 - val_accuracy: 0.8569\n",
            "Epoch 49/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0740 - accuracy: 0.9715 - val_loss: 0.6036 - val_accuracy: 0.8706\n",
            "Epoch 50/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.0726 - accuracy: 0.9726 - val_loss: 1.2362 - val_accuracy: 0.7915\n",
            "Epoch 51/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0758 - accuracy: 0.9717 - val_loss: 0.3691 - val_accuracy: 0.8807\n",
            "Epoch 52/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0708 - accuracy: 0.9728 - val_loss: 1.0590 - val_accuracy: 0.8533\n",
            "Epoch 53/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0703 - accuracy: 0.9732 - val_loss: 1.3836 - val_accuracy: 0.8569\n",
            "Epoch 54/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0675 - accuracy: 0.9748 - val_loss: 0.4465 - val_accuracy: 0.8850\n",
            "Epoch 55/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0706 - accuracy: 0.9731 - val_loss: 0.6656 - val_accuracy: 0.8792\n",
            "Epoch 56/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0710 - accuracy: 0.9734 - val_loss: 0.4952 - val_accuracy: 0.8835\n",
            "Epoch 57/150\n",
            "522/522 [==============================] - 20s 39ms/step - loss: 0.0682 - accuracy: 0.9743 - val_loss: 0.2862 - val_accuracy: 0.9087\n",
            "Epoch 58/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0644 - accuracy: 0.9753 - val_loss: 0.8681 - val_accuracy: 0.8814\n",
            "Epoch 59/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0638 - accuracy: 0.9757 - val_loss: 0.2894 - val_accuracy: 0.8843\n",
            "Epoch 60/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0627 - accuracy: 0.9761 - val_loss: 0.5297 - val_accuracy: 0.8073\n",
            "Epoch 61/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0672 - accuracy: 0.9745 - val_loss: 2.6846 - val_accuracy: 0.8116\n",
            "Epoch 62/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0669 - accuracy: 0.9746 - val_loss: 1.4113 - val_accuracy: 0.8771\n",
            "Epoch 63/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0567 - accuracy: 0.9785 - val_loss: 1.2725 - val_accuracy: 0.8720\n",
            "Epoch 64/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0607 - accuracy: 0.9771 - val_loss: 2.8315 - val_accuracy: 0.8850\n",
            "Epoch 65/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0575 - accuracy: 0.9786 - val_loss: 1.2014 - val_accuracy: 0.8332\n",
            "Epoch 66/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0596 - accuracy: 0.9776 - val_loss: 2.5644 - val_accuracy: 0.8879\n",
            "Epoch 67/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0544 - accuracy: 0.9799 - val_loss: 4.8008 - val_accuracy: 0.8138\n",
            "Epoch 68/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0504 - accuracy: 0.9807 - val_loss: 2.6433 - val_accuracy: 0.8843\n",
            "Epoch 69/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0545 - accuracy: 0.9796 - val_loss: 0.8933 - val_accuracy: 0.8354\n",
            "Epoch 70/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0550 - accuracy: 0.9798 - val_loss: 1.8668 - val_accuracy: 0.8684\n",
            "Epoch 71/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0513 - accuracy: 0.9808 - val_loss: 3.8726 - val_accuracy: 0.8131\n",
            "Epoch 72/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0520 - accuracy: 0.9801 - val_loss: 1.8272 - val_accuracy: 0.8124\n",
            "Epoch 73/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0517 - accuracy: 0.9810 - val_loss: 3.2259 - val_accuracy: 0.8641\n",
            "Epoch 74/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0541 - accuracy: 0.9801 - val_loss: 6.7634 - val_accuracy: 0.7872\n",
            "Epoch 75/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0509 - accuracy: 0.9812 - val_loss: 0.6245 - val_accuracy: 0.8641\n",
            "Epoch 76/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0496 - accuracy: 0.9814 - val_loss: 3.0325 - val_accuracy: 0.8231\n",
            "Epoch 77/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0462 - accuracy: 0.9826 - val_loss: 0.7625 - val_accuracy: 0.8196\n",
            "Epoch 78/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0501 - accuracy: 0.9816 - val_loss: 0.6140 - val_accuracy: 0.8835\n",
            "Epoch 79/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0480 - accuracy: 0.9815 - val_loss: 6.9524 - val_accuracy: 0.8167\n",
            "Epoch 80/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0484 - accuracy: 0.9820 - val_loss: 0.6091 - val_accuracy: 0.8634\n",
            "Epoch 81/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0460 - accuracy: 0.9826 - val_loss: 0.4842 - val_accuracy: 0.8994\n",
            "Epoch 82/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0480 - accuracy: 0.9820 - val_loss: 0.6906 - val_accuracy: 0.8958\n",
            "Epoch 83/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0501 - accuracy: 0.9812 - val_loss: 1.2119 - val_accuracy: 0.8943\n",
            "Epoch 84/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0438 - accuracy: 0.9833 - val_loss: 0.8180 - val_accuracy: 0.8756\n",
            "Epoch 85/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0486 - accuracy: 0.9818 - val_loss: 2.3128 - val_accuracy: 0.8347\n",
            "Epoch 86/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0422 - accuracy: 0.9840 - val_loss: 3.4219 - val_accuracy: 0.8217\n",
            "Epoch 87/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0444 - accuracy: 0.9831 - val_loss: 0.9505 - val_accuracy: 0.8900\n",
            "Epoch 88/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0424 - accuracy: 0.9840 - val_loss: 0.8147 - val_accuracy: 0.8735\n",
            "Epoch 89/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0416 - accuracy: 0.9842 - val_loss: 0.9449 - val_accuracy: 0.8799\n",
            "Epoch 90/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0437 - accuracy: 0.9833 - val_loss: 1.0906 - val_accuracy: 0.8267\n",
            "Epoch 91/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0423 - accuracy: 0.9843 - val_loss: 4.0345 - val_accuracy: 0.8821\n",
            "Epoch 92/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0358 - accuracy: 0.9867 - val_loss: 0.7008 - val_accuracy: 0.8922\n",
            "Epoch 93/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0437 - accuracy: 0.9842 - val_loss: 4.8253 - val_accuracy: 0.8440\n",
            "Epoch 94/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0405 - accuracy: 0.9855 - val_loss: 0.7516 - val_accuracy: 0.8828\n",
            "Epoch 95/150\n",
            "522/522 [==============================] - 21s 39ms/step - loss: 0.0409 - accuracy: 0.9848 - val_loss: 1.2937 - val_accuracy: 0.8728\n",
            "Epoch 96/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0364 - accuracy: 0.9865 - val_loss: 3.5133 - val_accuracy: 0.8756\n",
            "Epoch 97/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0380 - accuracy: 0.9858 - val_loss: 1.2310 - val_accuracy: 0.8785\n",
            "Epoch 98/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0386 - accuracy: 0.9857 - val_loss: 4.1886 - val_accuracy: 0.8167\n",
            "Epoch 99/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0431 - accuracy: 0.9843 - val_loss: 1.3631 - val_accuracy: 0.8907\n",
            "Epoch 100/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0392 - accuracy: 0.9855 - val_loss: 7.8586 - val_accuracy: 0.8203\n",
            "Epoch 101/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0377 - accuracy: 0.9857 - val_loss: 0.7914 - val_accuracy: 0.8325\n",
            "Epoch 102/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0363 - accuracy: 0.9869 - val_loss: 1.8591 - val_accuracy: 0.8670\n",
            "Epoch 103/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0388 - accuracy: 0.9860 - val_loss: 1.4310 - val_accuracy: 0.8239\n",
            "Epoch 104/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0371 - accuracy: 0.9858 - val_loss: 0.3425 - val_accuracy: 0.8864\n",
            "Epoch 105/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0369 - accuracy: 0.9866 - val_loss: 3.6190 - val_accuracy: 0.8210\n",
            "Epoch 106/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0368 - accuracy: 0.9863 - val_loss: 5.0975 - val_accuracy: 0.8311\n",
            "Epoch 107/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0346 - accuracy: 0.9873 - val_loss: 1.2336 - val_accuracy: 0.8138\n",
            "Epoch 108/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0345 - accuracy: 0.9871 - val_loss: 4.9269 - val_accuracy: 0.8871\n",
            "Epoch 109/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0352 - accuracy: 0.9867 - val_loss: 0.5408 - val_accuracy: 0.8692\n",
            "Epoch 110/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0326 - accuracy: 0.9883 - val_loss: 0.4331 - val_accuracy: 0.9044\n",
            "Epoch 111/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0378 - accuracy: 0.9863 - val_loss: 4.9063 - val_accuracy: 0.8196\n",
            "Epoch 112/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0379 - accuracy: 0.9869 - val_loss: 2.2067 - val_accuracy: 0.8095\n",
            "Epoch 113/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 0.9055 - val_accuracy: 0.9130\n",
            "Epoch 114/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0349 - accuracy: 0.9875 - val_loss: 0.5345 - val_accuracy: 0.9001\n",
            "Epoch 115/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0310 - accuracy: 0.9888 - val_loss: 1.0889 - val_accuracy: 0.8922\n",
            "Epoch 116/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0365 - accuracy: 0.9866 - val_loss: 0.7226 - val_accuracy: 0.8469\n",
            "Epoch 117/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0327 - accuracy: 0.9884 - val_loss: 2.9891 - val_accuracy: 0.8045\n",
            "Epoch 118/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0349 - accuracy: 0.9877 - val_loss: 2.3380 - val_accuracy: 0.8929\n",
            "Epoch 119/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0329 - accuracy: 0.9885 - val_loss: 6.9894 - val_accuracy: 0.8181\n",
            "Epoch 120/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0332 - accuracy: 0.9876 - val_loss: 4.1659 - val_accuracy: 0.8325\n",
            "Epoch 121/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0312 - accuracy: 0.9883 - val_loss: 4.1016 - val_accuracy: 0.8929\n",
            "Epoch 122/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0313 - accuracy: 0.9886 - val_loss: 1.6927 - val_accuracy: 0.8922\n",
            "Epoch 123/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0381 - accuracy: 0.9866 - val_loss: 2.1565 - val_accuracy: 0.8167\n",
            "Epoch 124/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0348 - accuracy: 0.9873 - val_loss: 2.0464 - val_accuracy: 0.8922\n",
            "Epoch 125/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0292 - accuracy: 0.9893 - val_loss: 2.1258 - val_accuracy: 0.8253\n",
            "Epoch 126/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0310 - accuracy: 0.9883 - val_loss: 6.4187 - val_accuracy: 0.8289\n",
            "Epoch 127/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0297 - accuracy: 0.9891 - val_loss: 0.8451 - val_accuracy: 0.8986\n",
            "Epoch 128/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0305 - accuracy: 0.9890 - val_loss: 1.7820 - val_accuracy: 0.8476\n",
            "Epoch 129/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0260 - accuracy: 0.9902 - val_loss: 4.2305 - val_accuracy: 0.8210\n",
            "Epoch 130/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0296 - accuracy: 0.9894 - val_loss: 0.8208 - val_accuracy: 0.8267\n",
            "Epoch 131/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0325 - accuracy: 0.9883 - val_loss: 1.7751 - val_accuracy: 0.8411\n",
            "Epoch 132/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0272 - accuracy: 0.9902 - val_loss: 0.6134 - val_accuracy: 0.9101\n",
            "Epoch 133/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0297 - accuracy: 0.9890 - val_loss: 4.1574 - val_accuracy: 0.8167\n",
            "Epoch 134/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0283 - accuracy: 0.9898 - val_loss: 3.4866 - val_accuracy: 0.8203\n",
            "Epoch 135/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0301 - accuracy: 0.9886 - val_loss: 2.6516 - val_accuracy: 0.8469\n",
            "Epoch 136/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0330 - accuracy: 0.9879 - val_loss: 0.4364 - val_accuracy: 0.8979\n",
            "Epoch 137/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0308 - accuracy: 0.9887 - val_loss: 4.0706 - val_accuracy: 0.8239\n",
            "Epoch 138/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0290 - accuracy: 0.9898 - val_loss: 0.5460 - val_accuracy: 0.9087\n",
            "Epoch 139/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0261 - accuracy: 0.9908 - val_loss: 1.4090 - val_accuracy: 0.8914\n",
            "Epoch 140/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0281 - accuracy: 0.9897 - val_loss: 1.7712 - val_accuracy: 0.8260\n",
            "Epoch 141/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0260 - accuracy: 0.9908 - val_loss: 0.6224 - val_accuracy: 0.8965\n",
            "Epoch 142/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 2.9983 - val_accuracy: 0.8181\n",
            "Epoch 143/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0298 - accuracy: 0.9896 - val_loss: 1.8499 - val_accuracy: 0.8792\n",
            "Epoch 144/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0246 - accuracy: 0.9909 - val_loss: 2.1645 - val_accuracy: 0.8914\n",
            "Epoch 145/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0252 - accuracy: 0.9911 - val_loss: 1.2680 - val_accuracy: 0.8490\n",
            "Epoch 146/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0259 - accuracy: 0.9905 - val_loss: 1.4511 - val_accuracy: 0.8785\n",
            "Epoch 147/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0293 - accuracy: 0.9893 - val_loss: 0.3957 - val_accuracy: 0.8763\n",
            "Epoch 148/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0289 - accuracy: 0.9899 - val_loss: 0.9728 - val_accuracy: 0.8850\n",
            "Epoch 149/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0259 - accuracy: 0.9905 - val_loss: 7.2680 - val_accuracy: 0.8217\n",
            "Epoch 150/150\n",
            "522/522 [==============================] - 21s 40ms/step - loss: 0.0251 - accuracy: 0.9907 - val_loss: 0.5944 - val_accuracy: 0.8699\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        },
        "id": "R6nm3sdFMm8e",
        "outputId": "9551781c-1084-4e79-cd41-ec172ef7c807"
      },
      "source": [
        "test_results = model.evaluate([x_test], [y_test, x_test])\n",
        "print(\"maximum train acc : \", max(history.history[\"accuracy\"]))\n",
        "print(\"maximum valid acc : \", max(history.history[\"val_accuracy\"]))\n",
        "print(\"test acc : \", test_results[1])\n",
        "print(\"test loss : \", test_results[0])\n",
        "\n",
        "\n",
        "y_pred = model.predict([x_test])\n",
        "score = f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average=\"macro\")\n",
        "print(\"f1 score : \", score)\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "\n",
        "label = [\"Jogging\", \"LyingDown\", \"Sitting\", \"Stairs\", \"Stading\", \"Walking\"]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(matrix, interpolation=\"nearest\")\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels(['']+label)\n",
        "ax.set_yticklabels(['']+label)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 1s 8ms/step - loss: 0.5097 - accuracy: 0.8843\n",
            "maximum train acc :  0.9911201000213623\n",
            "maximum valid acc :  0.9130122065544128\n",
            "test acc :  0.8843390941619873\n",
            "test loss :  0.5097446441650391\n",
            "f1 score :  0.8458126183240822\n",
            "[[200   0   0   0   0   0]\n",
            " [  0  79   2   1  52   0]\n",
            " [  0  16 231   0  68   3]\n",
            " [  0   0   0  21   0   1]\n",
            " [  0   1  12   0 123   0]\n",
            " [  1   0   1   1   2 577]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}