{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN-BN-LSTM.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNesDkwBZ/1LO5qRiW0SYmy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tolom131/Human-Activity-Recognition/blob/main/Tensorflow/only%20classifier_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5s9FOs7OKyE"
      },
      "source": [
        "# from __future__ import print_function\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import mean_squared_error, confusion_matrix, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, BatchNormalization, LSTM, Conv1D, Activation, MaxPooling1D, UpSampling1D, concatenate\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import initializers\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import random as rn\n",
        "from resource import *\n",
        "import time\n",
        "import math\n",
        "import sys\n",
        "from collections import Counter"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65m2C00d4uNJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "926ca97c-580a-4632-e485-34b4bdb8ee5e"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "directory_data  = './drive/MyDrive/HAR/'\n",
        "filename_data   = 'WISDM_at_v2.0_raw.txt'\n",
        "\n",
        "sys.path.append('/content/drive/MyDrive/HAR/')\n",
        "import wisdm_2_0\n",
        "# x_train, y_train, num_classes = wisdm_1_1.create_wisdm_1_1(directory_data + filename_data)\n",
        "origianl_x, original_y, num_classes = wisdm_2_0.create_wisdm_2_0(directory_data + filename_data)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: DtypeWarning: Columns (2) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_train.shape :  (13913, 200, 3) y_train.shape:  (13913, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-iVEk_mMtqi"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "def shift(x, y):\n",
        "    new = []\n",
        "    for i in range(5):\n",
        "        temp = np.roll(x, i*40, axis=1)\n",
        "        new.extend(temp)\n",
        "    x = np.array(new)\n",
        "    y = np.concatenate([y]*5)\n",
        "    x, y = shuffle(x, y, random_state=42)\n",
        "    return x,y\n",
        "\n",
        "from scipy.interpolate import CubicSpline\n",
        "def GenerateRandomCurves(X, sigma=0.2, knot=4):\n",
        "    xx = (np.ones((X.shape[1], 1))*(np.arange(0, X.shape[0], (X.shape[0]-1)/(knot+1)))).transpose()\n",
        "    yy = np.random.normal(loc=1.0, scale=sigma, size=(knot+2, X.shape[1]))\n",
        "    x_range = np.arange(X.shape[0])\n",
        "\n",
        "    cs_x = CubicSpline(xx[:, 0], yy[:, 0])\n",
        "    cs_y = CubicSpline(xx[:, 1], yy[:, 1])\n",
        "    cs_z = CubicSpline(xx[:, 2], yy[:, 2])\n",
        "    return np.array([cs_x(x_range), cs_y(x_range), cs_z(x_range)]).transpose()\n",
        "\n",
        "def DistortTimesteps(X, sigma=0.2):\n",
        "    tt = GenerateRandomCurves(X, sigma)\n",
        "    tt_cum = np.cumsum(tt, axis=0)\n",
        "\n",
        "    t_scale = [(X.shape[0]-1)/tt_cum[-1, 0], (X.shape[0]-1)/tt_cum[-1, 1], (X.shape[0]-1)/tt_cum[-1, 2]]\n",
        "    tt_cum[:, 0] = tt_cum[:, 0] * t_scale[0]\n",
        "    tt_cum[:, 1] = tt_cum[:, 1] * t_scale[1]\n",
        "    tt_cum[:, 2] = tt_cum[:, 2] * t_scale[2]\n",
        "\n",
        "    return tt_cum\n",
        "\n",
        "def DA_TimeWarp(X, sigma=0.2):\n",
        "    tt_new = DistortTimesteps(X, sigma)\n",
        "    X_new = np.zeros(X.shape)\n",
        "    x_range = np.arange(X.shape[0])\n",
        "\n",
        "    X_new[:, 0] = np.interp(x_range, tt_new[:, 0], X[:, 0])\n",
        "    X_new[:, 1] = np.interp(x_range, tt_new[:, 1], X[:, 1])\n",
        "    X_new[:, 2] = np.interp(x_range, tt_new[:, 2], X[:, 2])\n",
        "\n",
        "    return X_new\n",
        "\n",
        "def TimeWarp(X, Y):\n",
        "    for i in range(X.shape[0]):\n",
        "        data = X[i, :, :]\n",
        "        if i == 0:\n",
        "            trans_list = DA_TimeWarp(data).reshape(-1, X.shape[1], X.shape[2])\n",
        "        else:\n",
        "            trans = DA_TimeWarp(data).reshape(-1, X.shape[1], X.shape[2])\n",
        "            trans_list = np.concatenate([trans_list, trans], axis=0)\n",
        "\n",
        "    return trans_list, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSs2ExbNPOHD"
      },
      "source": [
        "def clssifier_without_ae(x_train, y_train, x_val, y_val):\n",
        "    \n",
        "    initializer = initializers.GlorotNormal()\n",
        "    earlystop = EarlyStopping(patience=50, monitor='val_loss', mode='min')\n",
        "    checkpoint = ModelCheckpoint(\"SAEwithout.h5\", verbose = 0, mode=\"min\", save_best_only=True, save_weights_only = True)\n",
        "    callbacks_list = [checkpoint ]\n",
        "    def make_adam(rates = 0.01):\n",
        "        return keras.optimizers.Adam(learning_rate=rates)\n",
        "\n",
        "    adam = make_adam()\n",
        "    inputs = Input(shape=(TIME_PERIODS, N_FEATURES))    \n",
        "    x = Conv1D(filters=16, kernel_size=3, kernel_initializer=initializer)(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = Conv1D(filters=32, kernel_size=3, kernel_initializer=initializer)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "    x = LSTM(128, kernel_initializer=initializer)(x)\n",
        "    classified = Dense(num_classes, activation=\"softmax\", name=\"classified\", kernel_initializer=initializer)(x)\n",
        "\n",
        "    model = Model(inputs, classified)\n",
        "    model.compile(loss=\"categorical_crossentropy\", optimizer=adam, metrics=['accuracy'])\n",
        "    history = model.fit(x_train, y_train, epochs=300, verbose=1, batch_size = 128, callbacks=callbacks_list, validation_data=(x_val, y_val))\n",
        "    return history, model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJ9h7-9R90kM"
      },
      "source": [
        "def build_resnet(input_shape, n_feature_maps, nb_classes):\n",
        "    x = keras.layers.Input(shape=(input_shape))\n",
        "    conv_x = keras.layers.BatchNormalization()(x)\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps, 8, 1, padding='same')(conv_x)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "     \n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps, 1, 1,padding='same')(x)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x)\n",
        "    print ('Merging skip connection')\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "     \n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    x1 = y\n",
        "    conv_x = keras.layers.Conv1D(n_feature_maps*2, 8, 1, padding='same')(x1)\n",
        "    conv_x = keras.layers.BatchNormalization()(conv_x)\n",
        "    conv_x = keras.layers.Activation('relu')(conv_x)\n",
        "     \n",
        "    conv_y = keras.layers.Conv1D(n_feature_maps*2, 5, 1, padding='same')(conv_x)\n",
        "    conv_y = keras.layers.BatchNormalization()(conv_y)\n",
        "    conv_y = keras.layers.Activation('relu')(conv_y)\n",
        "     \n",
        "    conv_z = keras.layers.Conv1D(n_feature_maps*2, 3, 1, padding='same')(conv_y)\n",
        "    conv_z = keras.layers.BatchNormalization()(conv_z)\n",
        "\n",
        "    is_expand_channels = not (input_shape[-1] == n_feature_maps*2)\n",
        "    if is_expand_channels:\n",
        "        shortcut_y = keras.layers.Conv1D(n_feature_maps*2, 1, 1,padding='same')(x1)\n",
        "        shortcut_y = keras.layers.BatchNormalization()(shortcut_y)\n",
        "    else:\n",
        "        shortcut_y = keras.layers.BatchNormalization()(x1)\n",
        "    y = keras.layers.Add()([shortcut_y, conv_z])\n",
        "    y = keras.layers.Activation('relu')(y)\n",
        "     \n",
        "    full = keras.layers.GlobalAveragePooling1D()(y)\n",
        "    out = keras.layers.Dense(nb_classes, activation='softmax', name=\"classified\")(full)\n",
        "    return x, out"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7-5ztN4VPT3m",
        "outputId": "9d9158e2-0b4f-4ec7-bf09-6905003c47f3"
      },
      "source": [
        "# fix the random seed\n",
        "# seed_sum = 42\n",
        "# np.random.seed(seed_sum)\n",
        "# rn.seed(seed_sum)\n",
        "# tf.random.set_seed(seed_sum)\n",
        "\n",
        "TIME_PERIODS = 200\n",
        "STEP = 200\n",
        "N_FEATURES = 3\n",
        "\n",
        "\n",
        "# 80%, 10%, 10%로 set 나누기\n",
        "x_train, x_val, y_train, y_val = train_test_split(origianl_x, original_y, test_size=0.2, stratify=original_y)\n",
        "x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size=0.5, stratify=y_val)\n",
        "\n",
        "# history, model = clssifier_without_ae(x_train, y_train, x_val, y_val)\n",
        "# model.load_weights(\"SAEwithout.h5\")\n",
        "# test_results = model.evaluate(x_test, y_test)\n",
        "\n",
        "x , y = build_resnet((200, 3), 64, 6)\n",
        "model = keras.models.Model(inputs=x, outputs=y)\n",
        "optimizer = keras.optimizers.Adam()\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "history = model.fit(x_train, y_train, batch_size=128, epochs=150, verbose=1, validation_data=(x_val, y_val))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merging skip connection\n",
            "Epoch 1/150\n",
            "87/87 [==============================] - 9s 51ms/step - loss: 0.6414 - accuracy: 0.7575 - val_loss: 2.4717 - val_accuracy: 0.1984\n",
            "Epoch 2/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.5171 - accuracy: 0.7863 - val_loss: 1.4385 - val_accuracy: 0.4752\n",
            "Epoch 3/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.4745 - accuracy: 0.8013 - val_loss: 1.4291 - val_accuracy: 0.4069\n",
            "Epoch 4/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.4588 - accuracy: 0.8097 - val_loss: 0.7958 - val_accuracy: 0.6643\n",
            "Epoch 5/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.4371 - accuracy: 0.8181 - val_loss: 0.6039 - val_accuracy: 0.7930\n",
            "Epoch 6/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.4265 - accuracy: 0.8210 - val_loss: 0.5039 - val_accuracy: 0.7779\n",
            "Epoch 7/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.4220 - accuracy: 0.8206 - val_loss: 0.4310 - val_accuracy: 0.8239\n",
            "Epoch 8/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.4108 - accuracy: 0.8247 - val_loss: 0.4997 - val_accuracy: 0.7793\n",
            "Epoch 9/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.4096 - accuracy: 0.8286 - val_loss: 0.4895 - val_accuracy: 0.7973\n",
            "Epoch 10/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.4027 - accuracy: 0.8267 - val_loss: 0.4703 - val_accuracy: 0.7973\n",
            "Epoch 11/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3920 - accuracy: 0.8313 - val_loss: 0.4882 - val_accuracy: 0.7743\n",
            "Epoch 12/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3858 - accuracy: 0.8304 - val_loss: 0.4464 - val_accuracy: 0.8167\n",
            "Epoch 13/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3926 - accuracy: 0.8290 - val_loss: 0.4900 - val_accuracy: 0.7750\n",
            "Epoch 14/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3806 - accuracy: 0.8364 - val_loss: 0.5217 - val_accuracy: 0.7951\n",
            "Epoch 15/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3727 - accuracy: 0.8371 - val_loss: 0.4624 - val_accuracy: 0.7973\n",
            "Epoch 16/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3714 - accuracy: 0.8403 - val_loss: 0.4312 - val_accuracy: 0.7915\n",
            "Epoch 17/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3622 - accuracy: 0.8422 - val_loss: 0.4142 - val_accuracy: 0.8037\n",
            "Epoch 18/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3697 - accuracy: 0.8414 - val_loss: 0.5519 - val_accuracy: 0.7728\n",
            "Epoch 19/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3517 - accuracy: 0.8468 - val_loss: 0.4355 - val_accuracy: 0.8095\n",
            "Epoch 20/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3638 - accuracy: 0.8413 - val_loss: 0.6051 - val_accuracy: 0.7965\n",
            "Epoch 21/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3598 - accuracy: 0.8463 - val_loss: 0.5635 - val_accuracy: 0.7973\n",
            "Epoch 22/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3354 - accuracy: 0.8532 - val_loss: 0.4642 - val_accuracy: 0.8231\n",
            "Epoch 23/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3374 - accuracy: 0.8495 - val_loss: 0.6057 - val_accuracy: 0.8030\n",
            "Epoch 24/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3452 - accuracy: 0.8495 - val_loss: 0.3857 - val_accuracy: 0.8497\n",
            "Epoch 25/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3325 - accuracy: 0.8550 - val_loss: 0.4149 - val_accuracy: 0.8145\n",
            "Epoch 26/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3350 - accuracy: 0.8534 - val_loss: 0.4131 - val_accuracy: 0.8368\n",
            "Epoch 27/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3321 - accuracy: 0.8528 - val_loss: 0.5538 - val_accuracy: 0.8081\n",
            "Epoch 28/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3272 - accuracy: 0.8539 - val_loss: 0.6762 - val_accuracy: 0.8095\n",
            "Epoch 29/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3260 - accuracy: 0.8573 - val_loss: 0.4106 - val_accuracy: 0.8275\n",
            "Epoch 30/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3171 - accuracy: 0.8586 - val_loss: 0.3519 - val_accuracy: 0.8648\n",
            "Epoch 31/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3066 - accuracy: 0.8659 - val_loss: 0.5289 - val_accuracy: 0.8210\n",
            "Epoch 32/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.3260 - accuracy: 0.8586 - val_loss: 0.4347 - val_accuracy: 0.8454\n",
            "Epoch 33/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3202 - accuracy: 0.8579 - val_loss: 0.5063 - val_accuracy: 0.8188\n",
            "Epoch 34/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3072 - accuracy: 0.8673 - val_loss: 0.4784 - val_accuracy: 0.8217\n",
            "Epoch 35/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3216 - accuracy: 0.8606 - val_loss: 1.7606 - val_accuracy: 0.7822\n",
            "Epoch 36/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2939 - accuracy: 0.8679 - val_loss: 0.4601 - val_accuracy: 0.8260\n",
            "Epoch 37/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.3034 - accuracy: 0.8691 - val_loss: 0.4384 - val_accuracy: 0.8203\n",
            "Epoch 38/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2992 - accuracy: 0.8682 - val_loss: 0.3596 - val_accuracy: 0.8548\n",
            "Epoch 39/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2976 - accuracy: 0.8750 - val_loss: 0.3994 - val_accuracy: 0.8490\n",
            "Epoch 40/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2902 - accuracy: 0.8764 - val_loss: 0.8062 - val_accuracy: 0.7894\n",
            "Epoch 41/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2868 - accuracy: 0.8776 - val_loss: 0.3847 - val_accuracy: 0.8397\n",
            "Epoch 42/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2826 - accuracy: 0.8773 - val_loss: 0.5671 - val_accuracy: 0.8181\n",
            "Epoch 43/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2794 - accuracy: 0.8801 - val_loss: 0.4228 - val_accuracy: 0.8497\n",
            "Epoch 44/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2714 - accuracy: 0.8826 - val_loss: 0.3946 - val_accuracy: 0.8289\n",
            "Epoch 45/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2737 - accuracy: 0.8785 - val_loss: 2.2250 - val_accuracy: 0.7958\n",
            "Epoch 46/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2740 - accuracy: 0.8872 - val_loss: 0.3336 - val_accuracy: 0.8613\n",
            "Epoch 47/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2735 - accuracy: 0.8819 - val_loss: 2.5678 - val_accuracy: 0.7944\n",
            "Epoch 48/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2632 - accuracy: 0.8898 - val_loss: 2.3179 - val_accuracy: 0.7987\n",
            "Epoch 49/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2528 - accuracy: 0.8936 - val_loss: 1.6064 - val_accuracy: 0.7829\n",
            "Epoch 50/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2701 - accuracy: 0.8854 - val_loss: 2.4494 - val_accuracy: 0.8023\n",
            "Epoch 51/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2595 - accuracy: 0.8925 - val_loss: 0.9134 - val_accuracy: 0.7886\n",
            "Epoch 52/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2695 - accuracy: 0.8876 - val_loss: 2.1266 - val_accuracy: 0.8081\n",
            "Epoch 53/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2499 - accuracy: 0.8913 - val_loss: 0.8884 - val_accuracy: 0.7958\n",
            "Epoch 54/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2430 - accuracy: 0.8986 - val_loss: 0.7180 - val_accuracy: 0.7894\n",
            "Epoch 55/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2514 - accuracy: 0.8978 - val_loss: 0.4327 - val_accuracy: 0.8023\n",
            "Epoch 56/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2510 - accuracy: 0.8979 - val_loss: 1.2750 - val_accuracy: 0.8009\n",
            "Epoch 57/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2464 - accuracy: 0.9014 - val_loss: 0.4209 - val_accuracy: 0.8713\n",
            "Epoch 58/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2386 - accuracy: 0.9009 - val_loss: 1.4110 - val_accuracy: 0.7944\n",
            "Epoch 59/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2389 - accuracy: 0.9040 - val_loss: 0.9887 - val_accuracy: 0.7944\n",
            "Epoch 60/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2418 - accuracy: 0.9002 - val_loss: 0.3585 - val_accuracy: 0.8483\n",
            "Epoch 61/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2452 - accuracy: 0.9027 - val_loss: 1.8138 - val_accuracy: 0.7894\n",
            "Epoch 62/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2340 - accuracy: 0.9048 - val_loss: 0.3768 - val_accuracy: 0.8648\n",
            "Epoch 63/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2213 - accuracy: 0.9144 - val_loss: 1.2782 - val_accuracy: 0.7937\n",
            "Epoch 64/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2213 - accuracy: 0.9156 - val_loss: 0.8035 - val_accuracy: 0.8167\n",
            "Epoch 65/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2254 - accuracy: 0.9115 - val_loss: 0.5729 - val_accuracy: 0.8648\n",
            "Epoch 66/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2361 - accuracy: 0.9038 - val_loss: 0.4281 - val_accuracy: 0.8562\n",
            "Epoch 67/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2255 - accuracy: 0.9098 - val_loss: 0.4666 - val_accuracy: 0.8684\n",
            "Epoch 68/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2209 - accuracy: 0.9128 - val_loss: 1.4765 - val_accuracy: 0.8131\n",
            "Epoch 69/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2147 - accuracy: 0.9182 - val_loss: 1.2811 - val_accuracy: 0.8095\n",
            "Epoch 70/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2105 - accuracy: 0.9197 - val_loss: 0.3908 - val_accuracy: 0.8605\n",
            "Epoch 71/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2305 - accuracy: 0.9128 - val_loss: 0.9344 - val_accuracy: 0.8167\n",
            "Epoch 72/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2179 - accuracy: 0.9162 - val_loss: 1.7371 - val_accuracy: 0.8001\n",
            "Epoch 73/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2227 - accuracy: 0.9119 - val_loss: 1.1143 - val_accuracy: 0.7793\n",
            "Epoch 74/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2174 - accuracy: 0.9190 - val_loss: 0.3556 - val_accuracy: 0.8613\n",
            "Epoch 75/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2070 - accuracy: 0.9186 - val_loss: 0.3596 - val_accuracy: 0.8562\n",
            "Epoch 76/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2185 - accuracy: 0.9151 - val_loss: 0.3178 - val_accuracy: 0.8677\n",
            "Epoch 77/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2118 - accuracy: 0.9193 - val_loss: 2.0040 - val_accuracy: 0.8059\n",
            "Epoch 78/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2032 - accuracy: 0.9219 - val_loss: 0.3153 - val_accuracy: 0.8541\n",
            "Epoch 79/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2057 - accuracy: 0.9229 - val_loss: 0.3777 - val_accuracy: 0.8598\n",
            "Epoch 80/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2034 - accuracy: 0.9212 - val_loss: 0.3478 - val_accuracy: 0.8605\n",
            "Epoch 81/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2204 - accuracy: 0.9169 - val_loss: 1.3585 - val_accuracy: 0.8081\n",
            "Epoch 82/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2086 - accuracy: 0.9216 - val_loss: 0.3099 - val_accuracy: 0.8814\n",
            "Epoch 83/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2126 - accuracy: 0.9173 - val_loss: 0.6700 - val_accuracy: 0.8670\n",
            "Epoch 84/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2140 - accuracy: 0.9184 - val_loss: 0.3780 - val_accuracy: 0.8548\n",
            "Epoch 85/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2049 - accuracy: 0.9225 - val_loss: 0.6762 - val_accuracy: 0.8160\n",
            "Epoch 86/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2086 - accuracy: 0.9209 - val_loss: 1.8318 - val_accuracy: 0.8030\n",
            "Epoch 87/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2018 - accuracy: 0.9243 - val_loss: 1.3290 - val_accuracy: 0.8174\n",
            "Epoch 88/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1966 - accuracy: 0.9249 - val_loss: 0.4251 - val_accuracy: 0.8613\n",
            "Epoch 89/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.1943 - accuracy: 0.9244 - val_loss: 1.8423 - val_accuracy: 0.8102\n",
            "Epoch 90/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1878 - accuracy: 0.9297 - val_loss: 0.4484 - val_accuracy: 0.8462\n",
            "Epoch 91/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2024 - accuracy: 0.9258 - val_loss: 0.4451 - val_accuracy: 0.8763\n",
            "Epoch 92/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1968 - accuracy: 0.9235 - val_loss: 1.1858 - val_accuracy: 0.7980\n",
            "Epoch 93/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2070 - accuracy: 0.9212 - val_loss: 0.8766 - val_accuracy: 0.8440\n",
            "Epoch 94/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2124 - accuracy: 0.9202 - val_loss: 0.4669 - val_accuracy: 0.8569\n",
            "Epoch 95/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1915 - accuracy: 0.9285 - val_loss: 1.1001 - val_accuracy: 0.8109\n",
            "Epoch 96/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.2018 - accuracy: 0.9231 - val_loss: 0.3827 - val_accuracy: 0.8634\n",
            "Epoch 97/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1920 - accuracy: 0.9292 - val_loss: 0.2440 - val_accuracy: 0.9209\n",
            "Epoch 98/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.1956 - accuracy: 0.9248 - val_loss: 1.2132 - val_accuracy: 0.8102\n",
            "Epoch 99/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1864 - accuracy: 0.9285 - val_loss: 0.7127 - val_accuracy: 0.8109\n",
            "Epoch 100/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1881 - accuracy: 0.9272 - val_loss: 0.2998 - val_accuracy: 0.8778\n",
            "Epoch 101/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.1870 - accuracy: 0.9279 - val_loss: 0.3629 - val_accuracy: 0.8692\n",
            "Epoch 102/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.2003 - accuracy: 0.9252 - val_loss: 0.5199 - val_accuracy: 0.8462\n",
            "Epoch 103/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1866 - accuracy: 0.9279 - val_loss: 0.8618 - val_accuracy: 0.8706\n",
            "Epoch 104/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1889 - accuracy: 0.9295 - val_loss: 1.1587 - val_accuracy: 0.8124\n",
            "Epoch 105/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1809 - accuracy: 0.9332 - val_loss: 1.4933 - val_accuracy: 0.8138\n",
            "Epoch 106/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1836 - accuracy: 0.9305 - val_loss: 0.3953 - val_accuracy: 0.8699\n",
            "Epoch 107/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1709 - accuracy: 0.9360 - val_loss: 0.5933 - val_accuracy: 0.8095\n",
            "Epoch 108/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1798 - accuracy: 0.9337 - val_loss: 0.3096 - val_accuracy: 0.8641\n",
            "Epoch 109/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1841 - accuracy: 0.9274 - val_loss: 0.6984 - val_accuracy: 0.8145\n",
            "Epoch 110/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1752 - accuracy: 0.9334 - val_loss: 0.9687 - val_accuracy: 0.8052\n",
            "Epoch 111/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1686 - accuracy: 0.9346 - val_loss: 1.1612 - val_accuracy: 0.8116\n",
            "Epoch 112/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1801 - accuracy: 0.9310 - val_loss: 0.4844 - val_accuracy: 0.8505\n",
            "Epoch 113/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1721 - accuracy: 0.9351 - val_loss: 0.4561 - val_accuracy: 0.8778\n",
            "Epoch 114/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1645 - accuracy: 0.9378 - val_loss: 0.4305 - val_accuracy: 0.8505\n",
            "Epoch 115/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1724 - accuracy: 0.9362 - val_loss: 0.5606 - val_accuracy: 0.8411\n",
            "Epoch 116/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1630 - accuracy: 0.9382 - val_loss: 0.4169 - val_accuracy: 0.8483\n",
            "Epoch 117/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1672 - accuracy: 0.9341 - val_loss: 1.0461 - val_accuracy: 0.8174\n",
            "Epoch 118/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1764 - accuracy: 0.9319 - val_loss: 0.5475 - val_accuracy: 0.8713\n",
            "Epoch 119/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1671 - accuracy: 0.9379 - val_loss: 0.4181 - val_accuracy: 0.8519\n",
            "Epoch 120/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1728 - accuracy: 0.9349 - val_loss: 0.6901 - val_accuracy: 0.8145\n",
            "Epoch 121/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1667 - accuracy: 0.9380 - val_loss: 0.4289 - val_accuracy: 0.8505\n",
            "Epoch 122/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1643 - accuracy: 0.9376 - val_loss: 0.7348 - val_accuracy: 0.8102\n",
            "Epoch 123/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1657 - accuracy: 0.9380 - val_loss: 0.7816 - val_accuracy: 0.8073\n",
            "Epoch 124/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1603 - accuracy: 0.9392 - val_loss: 0.3173 - val_accuracy: 0.9051\n",
            "Epoch 125/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1632 - accuracy: 0.9392 - val_loss: 0.3794 - val_accuracy: 0.8792\n",
            "Epoch 126/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1706 - accuracy: 0.9363 - val_loss: 0.3703 - val_accuracy: 0.8799\n",
            "Epoch 127/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1634 - accuracy: 0.9362 - val_loss: 0.4166 - val_accuracy: 0.8778\n",
            "Epoch 128/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1648 - accuracy: 0.9374 - val_loss: 0.9155 - val_accuracy: 0.8116\n",
            "Epoch 129/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1545 - accuracy: 0.9429 - val_loss: 0.9038 - val_accuracy: 0.7965\n",
            "Epoch 130/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1549 - accuracy: 0.9402 - val_loss: 0.5316 - val_accuracy: 0.8145\n",
            "Epoch 131/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1562 - accuracy: 0.9403 - val_loss: 0.7349 - val_accuracy: 0.8181\n",
            "Epoch 132/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1513 - accuracy: 0.9414 - val_loss: 0.7728 - val_accuracy: 0.8059\n",
            "Epoch 133/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1509 - accuracy: 0.9417 - val_loss: 0.6941 - val_accuracy: 0.8548\n",
            "Epoch 134/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1400 - accuracy: 0.9490 - val_loss: 1.0528 - val_accuracy: 0.8152\n",
            "Epoch 135/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1552 - accuracy: 0.9437 - val_loss: 0.6246 - val_accuracy: 0.8462\n",
            "Epoch 136/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1497 - accuracy: 0.9430 - val_loss: 0.4473 - val_accuracy: 0.8605\n",
            "Epoch 137/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1501 - accuracy: 0.9453 - val_loss: 0.3290 - val_accuracy: 0.8958\n",
            "Epoch 138/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1480 - accuracy: 0.9450 - val_loss: 0.6704 - val_accuracy: 0.8088\n",
            "Epoch 139/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1487 - accuracy: 0.9417 - val_loss: 0.5591 - val_accuracy: 0.8648\n",
            "Epoch 140/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.1510 - accuracy: 0.9442 - val_loss: 0.2759 - val_accuracy: 0.9029\n",
            "Epoch 141/150\n",
            "87/87 [==============================] - 4s 42ms/step - loss: 0.1409 - accuracy: 0.9482 - val_loss: 0.4281 - val_accuracy: 0.8663\n",
            "Epoch 142/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1411 - accuracy: 0.9474 - val_loss: 0.3885 - val_accuracy: 0.8613\n",
            "Epoch 143/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1466 - accuracy: 0.9441 - val_loss: 2.2739 - val_accuracy: 0.7994\n",
            "Epoch 144/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1476 - accuracy: 0.9438 - val_loss: 0.5459 - val_accuracy: 0.8728\n",
            "Epoch 145/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1359 - accuracy: 0.9514 - val_loss: 0.5615 - val_accuracy: 0.8814\n",
            "Epoch 146/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1459 - accuracy: 0.9467 - val_loss: 0.3559 - val_accuracy: 0.8706\n",
            "Epoch 147/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1401 - accuracy: 0.9483 - val_loss: 2.6804 - val_accuracy: 0.7980\n",
            "Epoch 148/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1405 - accuracy: 0.9465 - val_loss: 0.3262 - val_accuracy: 0.8807\n",
            "Epoch 149/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1407 - accuracy: 0.9471 - val_loss: 0.4031 - val_accuracy: 0.8605\n",
            "Epoch 150/150\n",
            "87/87 [==============================] - 4s 41ms/step - loss: 0.1376 - accuracy: 0.9489 - val_loss: 0.2698 - val_accuracy: 0.9094\n",
            "maximum train acc :  0.951392650604248\n",
            "maximum valid acc :  0.920920193195343\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-ebcd3a1edb2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maximum train acc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"maximum valid acc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"val_accuracy\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test acc : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test loss : \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_results' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R6nm3sdFMm8e",
        "outputId": "e6e209fb-2e44-4ca0-c503-043dcb9fe89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "test_results = model.evaluate([x_test], [y_test, x_test])\n",
        "print(\"maximum train acc : \", max(history.history[\"accuracy\"]))\n",
        "print(\"maximum valid acc : \", max(history.history[\"val_accuracy\"]))\n",
        "print(\"test acc : \", test_results[1])\n",
        "print(\"test loss : \", test_results[0])\n",
        "\n",
        "\n",
        "y_pred = model.predict([x_test])\n",
        "score = f1_score(y_test.argmax(axis=1), y_pred.argmax(axis=1), average=\"macro\")\n",
        "print(\"f1 score : \", score)\n",
        "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))\n",
        "print(matrix)\n",
        "\n",
        "label = [\"Jogging\", \"LyingDown\", \"Sitting\", \"Stairs\", \"Stading\", \"Walking\"]\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "cax = ax.matshow(matrix, interpolation=\"nearest\")\n",
        "fig.colorbar(cax)\n",
        "ax.set_xticklabels(['']+label)\n",
        "ax.set_yticklabels(['']+label)\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "44/44 [==============================] - 1s 8ms/step - loss: 0.2505 - accuracy: 0.9088\n",
            "maximum train acc :  0.951392650604248\n",
            "maximum valid acc :  0.920920193195343\n",
            "test acc :  0.9087643623352051\n",
            "test loss :  0.2505491077899933\n",
            "f1 score :  0.886952656977052\n",
            "[[196   0   0   0   2   2]\n",
            " [  0 123  10   0   1   0]\n",
            " [  3  48 254   0  10   3]\n",
            " [  0   0   0  20   0   2]\n",
            " [  1   2  21   0 103   9]\n",
            " [  1   0   7   1   4 569]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}